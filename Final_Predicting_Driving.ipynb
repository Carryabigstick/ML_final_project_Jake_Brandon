{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77ebe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main import block \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22125811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test data loaded successfully.\n",
      "Training data shape: (3644, 8)\n",
      "Testing data shape: (3084, 8)\n",
      "\n",
      "Original shape: (3644, 8)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_df = pd.read_csv('train_motion_data.csv')\n",
    "    test_df = pd.read_csv('test_motion_data.csv')\n",
    "    print(\"Train and test data loaded successfully.\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Testing data shape: {test_df.shape}\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'train_motion_data.csv' and 'test_motion_data.csv' are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "train_df\n",
    "\n",
    "print(f\"Original shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New approach \n",
    "\n",
    "def create_time_series(df, window_size):\n",
    "    \"\"\"\n",
    "    Creates time-series features based on a rolling window.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Creates magnitude vector for both acc and mag, to account for absolute changes \n",
    "    df['Acc_Mag'] = np.sqrt(df['AccX']**2 + df['AccY']**2 + df['AccZ']**2)\n",
    "    df['Gyro_Mag'] = np.sqrt(df['GyroX']**2 + df['GyroY']**2 + df['GyroZ']**2)\n",
    "\n",
    "    feature_cols = ['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ', 'Acc_Mag', 'Gyro_Mag']\n",
    "    # feature_cols = ['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']\n",
    "\n",
    "    df = df.sort_values(by='Timestamp').copy()\n",
    "\n",
    "    print(f\"\\n--- Engineering features with window size {window_size} ---\")\n",
    "    # df = df.sort_values(by='Timestamp').copy()\n",
    "\n",
    "\n",
    "    for col in feature_cols:\n",
    "        # .rolling() creates the window object.\n",
    "        # We then apply aggregate functions like .mean(), .std(), etc.\n",
    "        df[f'{col}_mean_{window_size}'] = df[col].rolling(window=window_size).mean()\n",
    "        df[f'{col}_std_{window_size}'] = df[col].rolling(window=window_size).std()\n",
    "        df[f'{col}_max_{window_size}'] = df[col].rolling(window=window_size).max()\n",
    "        df[f'{col}_min_{window_size}'] = df[col].rolling(window=window_size).min()\n",
    "        df[f'{col}_q75'] = df[col].rolling(window=window_size).quantile(0.75)\n",
    "        df[f'{col}_q25'] = df[col].rolling(window=window_size).quantile(0.25)\n",
    "\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160403f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Engineering features with window size 150 ---\n",
      "\n",
      "--- Engineering features with window size 150 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38807/488504228.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill')\n",
      "/tmp/ipykernel_38807/488504228.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the target and timestamp\n",
    "\n",
    "WINDOW_SIZE = 150   \n",
    "\n",
    "df_train = create_time_series(train_df, WINDOW_SIZE)\n",
    "df_test = create_time_series(test_df, WINDOW_SIZE)\n",
    "\n",
    "feature_cols = [c for c in df_train.columns if c not in ['Class', 'Timestamp']]\n",
    "target_col = 'Class'\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[target_col]\n",
    "X_test = df_test[feature_cols]\n",
    "y_test = df_test[target_col]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94dbce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features (Rolling Windows)...\n",
      "\n",
      "--- Engineering features with window size 150 ---\n",
      "\n",
      "--- Engineering features with window size 150 ---\n",
      "\n",
      "Starting Random Forest Tuning (Multithreaded)\n",
      "Testing 162 candidate models...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38807/488504228.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill')\n",
      "/tmp/ipykernel_38807/488504228.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill')\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 148 out of 162 | elapsed:   18.1s remaining:    1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search completed in 21.74 seconds.\n",
      "\n",
      "========================================\n",
      "       WINNING MODEL FOUND\n",
      "========================================\n",
      "Best Test Accuracy: 0.6443\n",
      "----------------------------------------\n",
      "Optimal Parameters:\n",
      "- n_estimators:      300\n",
      "- max_depth:         10.0\n",
      "- min_samples_split: 5\n",
      "- min_samples_leaf:  734\n",
      "- criterion:         entropy\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:   21.6s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from sklearn.calibration import Parallel, delayed\n",
    "\n",
    "\n",
    "print(\"Engineering features (Rolling Windows)...\")\n",
    "train_df_eng = create_time_series(train_df, WINDOW_SIZE)\n",
    "test_df_eng = create_time_series(test_df, WINDOW_SIZE)\n",
    "\n",
    "# Separate X (Features) and y (Target)\n",
    "feature_cols = [c for c in train_df_eng.columns if c not in ['Class', 'Timestamp']]\n",
    "X_train = train_df_eng[feature_cols]\n",
    "y_train = train_df_eng['Class']\n",
    "X_test = test_df_eng[feature_cols]\n",
    "y_test = test_df_eng['Class']\n",
    "\n",
    "# Scale the data (Standardizing helps Random Forest convergence slightly, critical for others)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 2. Define the Custom Grid Search Parameters ---\n",
    "\n",
    "print(\"\\nStarting Random Forest Tuning (Multithreaded)\")\n",
    "\n",
    "# Your requested parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 723],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 20],\n",
    "    'min_samples_leaf': [1, 4, 734],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Generate all possible combinations of parameters\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"Testing {len(param_combinations)} candidate models...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- 3. Define the Worker Function ---\n",
    "\n",
    "def evaluate_candidate(params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a single model and evaluates it on the test set.\n",
    "    This function will be run in parallel across multiple CPU cores.\n",
    "    \"\"\"\n",
    "    # CRITICAL: n_jobs=1 ensures this specific model uses only 1 core.\n",
    "    # We rely on the outer joblib loop to parallelize across cores.\n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=1, **params)\n",
    "    \n",
    "    # Train\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = rf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Return dictionary of results\n",
    "    result = params.copy()\n",
    "    result['test_accuracy'] = acc\n",
    "    return result\n",
    "\n",
    "# --- 4. Run Parallel Execution ---\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# n_jobs=-1 uses all available cores. verbose=10 shows progress bar.\n",
    "results = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(evaluate_candidate)(params, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    for params in param_combinations\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nGrid Search completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 5. Analyze and Report Results ---\n",
    "\n",
    "# Convert to DataFrame for easy sorting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by Test Accuracy (Descending)\n",
    "best_model = results_df.sort_values(by='test_accuracy', ascending=False).iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"       WINNING MODEL FOUND\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Best Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Optimal Parameters:\")\n",
    "print(f\"- n_estimators:      {best_model['n_estimators']}\")\n",
    "print(f\"- max_depth:         {best_model['max_depth']}\")\n",
    "print(f\"- min_samples_split: {best_model['min_samples_split']}\")\n",
    "print(f\"- min_samples_leaf:  {best_model['min_samples_leaf']}\")\n",
    "print(f\"- criterion:         {best_model['criterion']}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ad233c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting random Forest Tuning\n",
      "\n",
      "Starting Custom Grid Search on 162 candidates...\n",
      "n_est      depth      split      leaf       crit       | TEST ACCURACY\n",
      "--------------------------------------------------------------------------------\n",
      "200        10         2          1          gini       | 0.5435\n",
      "200        10         2          1          entropy    | 0.5691\n",
      "200        10         2          4          gini       | 0.5496\n",
      "200        10         2          4          entropy    | 0.5976\n",
      "200        10         2          734        gini       | 0.6274\n",
      "200        10         2          734        entropy    | 0.6378\n",
      "200        10         5          1          gini       | 0.5295\n",
      "200        10         5          1          entropy    | 0.5519\n",
      "200        10         5          4          gini       | 0.5496\n",
      "200        10         5          4          entropy    | 0.5976\n",
      "200        10         5          734        gini       | 0.6274\n",
      "200        10         5          734        entropy    | 0.6378\n",
      "200        10         20         1          gini       | 0.5674\n",
      "200        10         20         1          entropy    | 0.5486\n",
      "200        10         20         4          gini       | 0.5746\n",
      "200        10         20         4          entropy    | 0.6012\n",
      "200        10         20         734        gini       | 0.6274\n",
      "200        10         20         734        entropy    | 0.6378\n",
      "200        20         2          1          gini       | 0.5373\n",
      "200        20         2          1          entropy    | 0.5678\n",
      "200        20         2          4          gini       | 0.5431\n",
      "200        20         2          4          entropy    | 0.5989\n",
      "200        20         2          734        gini       | 0.6274\n",
      "200        20         2          734        entropy    | 0.6378\n",
      "200        20         5          1          gini       | 0.5295\n",
      "200        20         5          1          entropy    | 0.5519\n",
      "200        20         5          4          gini       | 0.5431\n",
      "200        20         5          4          entropy    | 0.5989\n",
      "200        20         5          734        gini       | 0.6274\n",
      "200        20         5          734        entropy    | 0.6378\n",
      "200        20         20         1          gini       | 0.5593\n",
      "200        20         20         1          entropy    | 0.5477\n",
      "200        20         20         4          gini       | 0.6187\n",
      "200        20         20         4          entropy    | 0.5970\n",
      "200        20         20         734        gini       | 0.6274\n",
      "200        20         20         734        entropy    | 0.6378\n",
      "200        None       2          1          gini       | 0.5373\n",
      "200        None       2          1          entropy    | 0.5678\n",
      "200        None       2          4          gini       | 0.5431\n",
      "200        None       2          4          entropy    | 0.5989\n",
      "200        None       2          734        gini       | 0.6274\n",
      "200        None       2          734        entropy    | 0.6378\n",
      "200        None       5          1          gini       | 0.5295\n",
      "200        None       5          1          entropy    | 0.5519\n",
      "200        None       5          4          gini       | 0.5431\n",
      "200        None       5          4          entropy    | 0.5989\n",
      "200        None       5          734        gini       | 0.6274\n",
      "200        None       5          734        entropy    | 0.6378\n",
      "200        None       20         1          gini       | 0.5593\n",
      "200        None       20         1          entropy    | 0.5477\n",
      "200        None       20         4          gini       | 0.6187\n",
      "200        None       20         4          entropy    | 0.5970\n",
      "200        None       20         734        gini       | 0.6274\n",
      "200        None       20         734        entropy    | 0.6378\n",
      "300        10         2          1          gini       | 0.5431\n",
      "300        10         2          1          entropy    | 0.5493\n",
      "300        10         2          4          gini       | 0.5519\n",
      "300        10         2          4          entropy    | 0.5872\n",
      "300        10         2          734        gini       | 0.6333\n",
      "300        10         2          734        entropy    | 0.6443\n",
      "300        10         5          1          gini       | 0.5360\n",
      "300        10         5          1          entropy    | 0.5396\n",
      "300        10         5          4          gini       | 0.5519\n",
      "300        10         5          4          entropy    | 0.5872\n",
      "300        10         5          734        gini       | 0.6333\n",
      "300        10         5          734        entropy    | 0.6443\n",
      "300        10         20         1          gini       | 0.5593\n",
      "300        10         20         1          entropy    | 0.5363\n",
      "300        10         20         4          gini       | 0.5772\n",
      "300        10         20         4          entropy    | 0.5963\n",
      "300        10         20         734        gini       | 0.6333\n",
      "300        10         20         734        entropy    | 0.6443\n",
      "300        20         2          1          gini       | 0.5402\n",
      "300        20         2          1          entropy    | 0.5467\n",
      "300        20         2          4          gini       | 0.5486\n",
      "300        20         2          4          entropy    | 0.5869\n",
      "300        20         2          734        gini       | 0.6333\n",
      "300        20         2          734        entropy    | 0.6443\n",
      "300        20         5          1          gini       | 0.5292\n",
      "300        20         5          1          entropy    | 0.5447\n",
      "300        20         5          4          gini       | 0.5486\n",
      "300        20         5          4          entropy    | 0.5869\n",
      "300        20         5          734        gini       | 0.6333\n",
      "300        20         5          734        entropy    | 0.6443\n",
      "300        20         20         1          gini       | 0.5493\n",
      "300        20         20         1          entropy    | 0.5366\n",
      "300        20         20         4          gini       | 0.5697\n",
      "300        20         20         4          entropy    | 0.5512\n",
      "300        20         20         734        gini       | 0.6333\n",
      "300        20         20         734        entropy    | 0.6443\n",
      "300        None       2          1          gini       | 0.5402\n",
      "300        None       2          1          entropy    | 0.5467\n",
      "300        None       2          4          gini       | 0.5486\n",
      "300        None       2          4          entropy    | 0.5869\n",
      "300        None       2          734        gini       | 0.6333\n",
      "300        None       2          734        entropy    | 0.6443\n",
      "300        None       5          1          gini       | 0.5292\n",
      "300        None       5          1          entropy    | 0.5447\n",
      "300        None       5          4          gini       | 0.5486\n",
      "300        None       5          4          entropy    | 0.5869\n",
      "300        None       5          734        gini       | 0.6333\n",
      "300        None       5          734        entropy    | 0.6443\n",
      "300        None       20         1          gini       | 0.5493\n",
      "300        None       20         1          entropy    | 0.5366\n",
      "300        None       20         4          gini       | 0.5697\n",
      "300        None       20         4          entropy    | 0.5512\n",
      "300        None       20         734        gini       | 0.6333\n",
      "300        None       20         734        entropy    | 0.6443\n",
      "723        10         2          1          gini       | 0.5315\n",
      "723        10         2          1          entropy    | 0.5357\n",
      "723        10         2          4          gini       | 0.5399\n",
      "723        10         2          4          entropy    | 0.5454\n",
      "723        10         2          734        gini       | 0.6180\n",
      "723        10         2          734        entropy    | 0.6404\n",
      "723        10         5          1          gini       | 0.5188\n",
      "723        10         5          1          entropy    | 0.5331\n",
      "723        10         5          4          gini       | 0.5399\n",
      "723        10         5          4          entropy    | 0.5454\n",
      "723        10         5          734        gini       | 0.6180\n",
      "723        10         5          734        entropy    | 0.6404\n",
      "723        10         20         1          gini       | 0.5357\n",
      "723        10         20         1          entropy    | 0.5415\n",
      "723        10         20         4          gini       | 0.5535\n",
      "723        10         20         4          entropy    | 0.5901\n",
      "723        10         20         734        gini       | 0.6180\n",
      "723        10         20         734        entropy    | 0.6404\n",
      "723        20         2          1          gini       | 0.5263\n",
      "723        20         2          1          entropy    | 0.5366\n",
      "723        20         2          4          gini       | 0.5347\n",
      "723        20         2          4          entropy    | 0.5444\n",
      "723        20         2          734        gini       | 0.6180\n",
      "723        20         2          734        entropy    | 0.6404\n",
      "723        20         5          1          gini       | 0.5149\n",
      "723        20         5          1          entropy    | 0.5353\n",
      "723        20         5          4          gini       | 0.5347\n",
      "723        20         5          4          entropy    | 0.5444\n",
      "723        20         5          734        gini       | 0.6180\n",
      "723        20         5          734        entropy    | 0.6404\n",
      "723        20         20         1          gini       | 0.5386\n",
      "723        20         20         1          entropy    | 0.5412\n",
      "723        20         20         4          gini       | 0.5535\n",
      "723        20         20         4          entropy    | 0.5908\n",
      "723        20         20         734        gini       | 0.6180\n",
      "723        20         20         734        entropy    | 0.6404\n",
      "723        None       2          1          gini       | 0.5263\n",
      "723        None       2          1          entropy    | 0.5366\n",
      "723        None       2          4          gini       | 0.5347\n",
      "723        None       2          4          entropy    | 0.5444\n",
      "723        None       2          734        gini       | 0.6180\n",
      "723        None       2          734        entropy    | 0.6404\n",
      "723        None       5          1          gini       | 0.5149\n",
      "723        None       5          1          entropy    | 0.5353\n",
      "723        None       5          4          gini       | 0.5347\n",
      "723        None       5          4          entropy    | 0.5444\n",
      "723        None       5          734        gini       | 0.6180\n",
      "723        None       5          734        entropy    | 0.6404\n",
      "723        None       20         1          gini       | 0.5386\n",
      "723        None       20         1          entropy    | 0.5412\n",
      "723        None       20         4          gini       | 0.5535\n",
      "723        None       20         4          entropy    | 0.5908\n",
      "723        None       20         734        gini       | 0.6180\n",
      "723        None       20         734        entropy    | 0.6404\n",
      "\n",
      "==============================\n",
      "     WINNING MODEL FOUND\n",
      "==============================\n",
      "Best Test Accuracy: 0.6443\n",
      "Parameters:\n",
      "- n_estimators: 300\n",
      "- max_depth: 10.0\n",
      "- min_samples_split: 5\n",
      "- min_samples_leaf: 734\n",
      "- criterion: entropy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "print(\"Starting random Forest Tuning\")\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],        # Number of trees\n",
    "#     'max_depth': [None, 10, 20, 30],        # Max depth of each tree\n",
    "#     'min_samples_split': [2, 5, 10],        # Min samples to split a node\n",
    "#     'min_samples_leaf': [1, 2, 4],          # Min samples at a leaf node\n",
    "#     'bootstrap': [True, False],             # Method of selecting samples\n",
    "#     'criterion': ['gini', 'entropy']        # Function to measure split quality\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300,723],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 20],\n",
    "    'min_samples_leaf': [1, 4, 734],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "# Generate all possible combinations of parameters\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"\\nStarting Custom Grid Search on {len(param_combinations)} candidates...\")\n",
    "print(f\"{'n_est':<10} {'depth':<10} {'split':<10} {'leaf':<10} {'crit':<10} | {'TEST ACCURACY'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through every combination\n",
    "for params in param_combinations:\n",
    "    # A. Configure Model\n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1, **params)\n",
    "    \n",
    "    # B. Train on FULL Training Set\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # C. Evaluate on FULL Test Set\n",
    "    predictions = rf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # D. Save Results\n",
    "    result = params.copy()\n",
    "    result['test_accuracy'] = acc\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress row\n",
    "    print(f\"{params['n_estimators']:<10} {str(params['max_depth']):<10} {params['min_samples_split']:<10} {params['min_samples_leaf']:<10} {params['criterion']:<10} | {acc:.4f}\")\n",
    "    # print(f\"{params['n_estimators']:<10} {str(params['max_depth']):<10} {params['min_samples_leaf']:<10} | {acc:.4f}\")\n",
    "\n",
    "\n",
    "# --- 3. Analyze Results ---\n",
    "\n",
    "# Convert to DataFrame for easy sorting\n",
    "results_df = pd.DataFrame(results)\n",
    "best_model = results_df.sort_values(by='test_accuracy', ascending=False).iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"     WINNING MODEL FOUND\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Best Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "print(\"Parameters:\")\n",
    "print(f\"- n_estimators: {best_model['n_estimators']}\")\n",
    "print(f\"- max_depth: {best_model['max_depth']}\")\n",
    "print(f\"- min_samples_split: {best_model['min_samples_split']}\")\n",
    "print(f\"- min_samples_leaf: {best_model['min_samples_leaf']}\")\n",
    "print(f\"- criterion: {best_model['criterion']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0fa908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest on 40 features...\n",
      "\n",
      "--- Results ---\n",
      "Training Accuracy: 0.6084\n",
      "Testing Accuracy:  0.5798\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  AGGRESSIVE       0.55      0.85      0.67       814\n",
      "      NORMAL       0.00      0.00      0.00       997\n",
      "        SLOW       0.60      0.86      0.71      1273\n",
      "\n",
      "    accuracy                           0.58      3084\n",
      "   macro avg       0.38      0.57      0.46      3084\n",
      "weighted avg       0.39      0.58      0.47      3084\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "AccY_std         0.184077\n",
      "Acc_Mag_mean     0.153880\n",
      "Acc_Mag_std      0.119329\n",
      "AccY_min         0.080352\n",
      "Acc_Mag_max      0.080292\n",
      "AccZ_min         0.054612\n",
      "Gyro_Mag_mean    0.050729\n",
      "AccY_max         0.041786\n",
      "AccX_std         0.037195\n",
      "GyroZ_min        0.033456\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=723,\n",
    "    max_depth=None,         # Limits tree complexity\n",
    "    min_samples_leaf=734,   # Requires at least 4 samples to make a decision\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"Training Random Forest on {X_train_scaled.shape[1]} features...\")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# 4. Evaluate\n",
    "train_pred = rf_model.predict(X_train_scaled)\n",
    "test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, train_pred):.4f}\")\n",
    "print(f\"Testing Accuracy:  {accuracy_score(y_test, test_pred):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "\n",
    "# 5. Feature Importance Check\n",
    "# See which features actually mattered\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_cols)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importances.nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c23cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Random Forest Tuning (Ranking by TEST accuracy - Takes like 8 minutes with current params be warned)... ---\n",
      "\n",
      "Best Parameters Based on TEST Accuracy:\n",
      "{'max_depth': None, 'min_samples_leaf': 734, 'n_estimators': 723}\n",
      "Best TEST Accuracy: 0.5798\n",
      "\n",
      "Classification Report for Best TEST-Selected Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  AGGRESSIVE       0.55      0.85      0.67       814\n",
      "      NORMAL       0.00      0.00      0.00       997\n",
      "        SLOW       0.60      0.86      0.71      1273\n",
      "\n",
      "    accuracy                           0.58      3084\n",
      "   macro avg       0.38      0.57      0.46      3084\n",
      "weighted avg       0.39      0.58      0.47      3084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "\n",
    "print(\"\\n--- Starting Random Forest Tuning (Ranking by TEST accuracy - Takes like 8 minutes with current params be warned)... ---\")\n",
    "\n",
    "rf_param_grid_regularized = {\n",
    "    'n_estimators': [723],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_leaf': [734]\n",
    "}\n",
    "# 'n_estimators': [50, 100, 200, 300],\n",
    "#    'max_depth': [3, 5, 8, 13],\n",
    "#   'min_samples_leaf': [10, 20, 50, 100, 150, 200]\n",
    "#Best Parameters Based on TEST Accuracy:\n",
    "#{'max_depth': 3, 'min_samples_leaf': 900, 'n_estimators': 500}\n",
    "#{'max_depth': 3, 'min_samples_leaf': 734, 'n_estimators': 723}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=rf_param_grid_regularized,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Rank models based only on test-set performance\n",
    "test_accuracies = []\n",
    "candidates = rf_grid.cv_results_[\"params\"]\n",
    "\n",
    "for params in candidates:\n",
    "    # Train new model with these params on *full training set*\n",
    "    model = RandomForestClassifier(random_state=42, **params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate on TEST set\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    test_accuracies.append((params, test_acc))\n",
    "\n",
    "# Sort by best test accuracy\n",
    "test_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "best_params_test, best_test_accuracy = test_accuracies[0]\n",
    "\n",
    "print(\"\\nBest Parameters Based on TEST Accuracy:\")\n",
    "print(best_params_test)\n",
    "print(f\"Best TEST Accuracy: {best_test_accuracy:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_model = RandomForestClassifier(random_state=42, **best_params_test)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nClassification Report for Best TEST-Selected Model:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
